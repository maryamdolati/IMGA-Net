{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx-dTwg7jKqL"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy.sparse import lil_matrix, save_npz, load_npz\n",
        "import h5py\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.sparse import load_npz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "directory_path = r\"D:\\New folder (5)\"\n",
        "embeddings_dir = os.path.join(directory_path, 'embeddings')\n",
        "gcn_file_path = os.path.join(directory_path, 'node_embeddings_gcn16.npy')\n",
        "labels_file_path = os.path.join(directory_path, 'label.txt')\n",
        "source_tweets_file = r\"D:\\New folder (5)\\source_tweets.txt\"\n",
        "dependency_matrix_path = 'dependency_matrix16.npz'\n",
        "pmi_matrix_path = 'pmi_matrix16.npz'\n",
        "tweets_file_path = r\"D:\\New folder (5)\\source_tweets.txt\"\n",
        "tree_folder_path = r\"D:\\New folder (5)\\tree\"\n",
        "embeddings_dir = r\"D:\\New folder (5)\\embeddings\"\n",
        "output_dir = r\"D:\\New folder (5)\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "def load_and_tokenize(file_path, tokenizer):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        tweets = [line.strip() for line in file.readlines()]\n",
        "    token_ids = []\n",
        "    skipgram_counts = Counter()\n",
        "    for tweet in tweets:\n",
        "        tokens = tokenizer.tokenize(tweet)\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        token_ids.extend(indexed_tokens)\n",
        "        for i in range(len(indexed_tokens)):\n",
        "            for j in range(max(0, i-2), min(len(indexed_tokens), i+3)):\n",
        "                if i != j:\n",
        "                    skipgram_counts[(indexed_tokens[i], indexed_tokens[j])] += 1\n",
        "    return token_ids, skipgram_counts\n",
        "\n",
        "file_path = r\"C:\\Users\\AI-BIO\\Untitled Folder\\source_tweets.txt\"\n",
        "token_ids, skipgram_counts = load_and_tokenize(file_path, tokenizer)\n",
        "\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "total_skipgrams = sum(skipgram_counts.values())\n",
        "sum_over_words = np.zeros(vocab_size)\n",
        "sum_over_contexts = np.zeros(vocab_size)\n",
        "for token_id in token_ids:\n",
        "    sum_over_words[token_id] += 1\n",
        "sum_over_contexts = sum_over_words ** 0.75\n",
        "nca_denom = sum(sum_over_contexts)\n",
        "\n",
        "pmi_values, spmi_values = [], []\n",
        "rows, cols = [], []\n",
        "\n",
        "for (tok1, tok2), count in skipgram_counts.items():\n",
        "    Pwc = count / total_skipgrams\n",
        "    Pw = sum_over_words[tok1] / total_skipgrams\n",
        "    Pc = sum_over_words[tok2] / total_skipgrams\n",
        "    Pca = sum_over_contexts[tok2] / nca_denom\n",
        "\n",
        "    pmi = np.log2(Pwc / (Pw * Pc)) if Pwc > 0 else 0\n",
        "    spmi = np.log2(Pwc / (Pw * Pca)) if Pwc > 0 else 0\n",
        "    ppmi = max(pmi, 0)\n",
        "    sppmi = max(spmi, 0)\n",
        "\n",
        "    rows.append(tok1)\n",
        "    cols.append(tok2)\n",
        "    pmi_values.append(pmi)\n",
        "    spmi_values.append(spmi)\n",
        "\n",
        "\n",
        "pmi_mat = sparse.csr_matrix((pmi_values, (rows, cols)), shape=(vocab_size, vocab_size))\n",
        "spmi_mat = sparse.csr_matrix((spmi_values, (rows, cols)), shape=(vocab_size, vocab_size))\n",
        "\n",
        "print(\"Vocabulary size from BERT tokenizer:\", vocab_size)\n",
        "print(\"Shape of PMI matrix:\", pmi_mat.shape)\n",
        "print(\"Shape of SPMI matrix:\", spmi_mat.shape)\n",
        "\n",
        "\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "model_bert.eval()\n",
        "\n",
        "\n",
        "pmi_coo = pmi_mat.tocoo()\n",
        "\n",
        "\n",
        "edge_index = np.vstack((pmi_coo.row, pmi_coo.col))\n",
        "edge_weight = np.array(pmi_coo.data, dtype=np.float32)\n",
        "\n",
        "\n",
        "data_pmi = Data(edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
        "                edge_attr=torch.tensor(edge_weight, dtype=torch.float))\n",
        "from scipy import sparse\n",
        "import torch\n",
        "\n",
        "\n",
        "sparse.save_npz('pmi_matrix.npz', pmi_mat)\n",
        "sparse.save_npz('spmi_matrix.npz', spmi_mat)\n",
        "torch.save(data_pmi, 'pmi_graph_data.pt')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def create_dependency_matrix(tweets):\n",
        "    dependency_counts = defaultdict(int)\n",
        "    total_dependencies = 0\n",
        "\n",
        "    for tweet in tqdm(tweets, desc=\"Processing tweets\"):\n",
        "        tokens = tweet.split()\n",
        "        for i, token in enumerate(tokens):\n",
        "            if i > 0:\n",
        "                dependency_counts[(tokens[i-1], token)] += 1\n",
        "                total_dependencies += 1\n",
        "            if i < len(tokens) - 1:\n",
        "                dependency_counts[(token, tokens[i+1])] += 1\n",
        "                total_dependencies += 1\n",
        "\n",
        "    return dependency_counts, total_dependencies\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        tweets = [preprocess_text(line.strip()) for line in file.readlines()]\n",
        "    return tweets\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_features, 64, heads=2, concat=True)\n",
        "        self.conv2 = GATConv(64 * 2, out_features, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return x\n",
        "\n",
        "\n",
        "dependency_counts, total_dependencies = create_dependency_matrix(tweets)\n",
        "\n",
        "\n",
        "rows, cols, data = [], [], []\n",
        "for (tok1, tok2), count in tqdm(dependency_counts.items(), desc=\"Building sparse matrix\"):\n",
        "    rows.append(tokenizer.convert_tokens_to_ids([tok1])[0])\n",
        "    cols.append(tokenizer.convert_tokens_to_ids([tok2])[0])\n",
        "    data.append(count / total_dependencies)\n",
        "\n",
        "dependency_mat = sparse.csr_matrix((data, (rows, cols)), shape=(tokenizer.vocab_size, tokenizer.vocab_size))\n",
        "\n",
        "\n",
        "dependency_coo = dependency_mat.tocoo()\n",
        "\n",
        "\n",
        "edge_index_dep = np.vstack((dependency_coo.row, dependency_coo.col))\n",
        "edge_weight_dep = np.array(dependency_coo.data, dtype=np.float32)\n",
        "\n",
        "\n",
        "data_dep = Data(edge_index=torch.tensor(edge_index_dep, dtype=torch.long),\n",
        "                edge_attr=torch.tensor(edge_weight_dep, dtype=torch.float))\n",
        "\n",
        "\n",
        "def load_embeddings(tokenizer, model):\n",
        "    embeddings = []\n",
        "    for token_id in tqdm(range(tokenizer.vocab_size), desc=\"Loading embeddings\"):\n",
        "        token = tokenizer.convert_ids_to_tokens([token_id])\n",
        "        encoded_input = tokenizer(token, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded_input)\n",
        "        embeddings.append(output.last_hidden_state[:, 0, :].squeeze().cpu().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "from transformers import BertModel\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "embeddings = load_embeddings(tokenizer, model_bert)\n",
        "\n",
        "\n",
        "node_features = torch.tensor(embeddings, dtype=torch.float32)\n",
        "from scipy import sparse\n",
        "import torch\n",
        "\n",
        "\n",
        "sparse.save_npz('dependency_matrix16.npz', dependency_mat)\n",
        "\n",
        "torch.save(data_dep, 'dependency_graph_data16.pt')\n",
        "\n",
        "torch.save(node_features, 'node_features16.pt')\n",
        "\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "tweets_df = pd.read_csv(tweets_file_path, sep='\\t', header=None, names=['tweet_id', 'tweet_text'])\n",
        "\n",
        "tree_files = [os.path.join(tree_folder_path, file_name) for file_name in os.listdir(tree_folder_path) if file_name.endswith('.txt')]\n",
        "\n",
        "\n",
        "graph_data = defaultdict(list)\n",
        "for file_path in tqdm(tree_files, desc=\"Processing Tree Files\"):\n",
        "    with open(file_path, 'r') as file:\n",
        "        parent_id = os.path.splitext(os.path.basename(file_path))[0]  # Parent tweet ID from file name\n",
        "        for line in file:\n",
        "            child_id = line.strip()\n",
        "            graph_data[parent_id].append(child_id)\n",
        "\n",
        "\n",
        "all_tweet_ids = list(set(graph_data.keys()).union(set(child_id for children in graph_data.values() for child_id in children)))\n",
        "\n",
        "all_tweet_ids = all_tweet_ids[:75990]\n",
        "\n",
        "\n",
        "tweet_id_to_index = {tweet_id: idx for idx, tweet_id in enumerate(all_tweet_ids)}\n",
        "\n",
        "\n",
        "num_tweets = len(all_tweet_ids)\n",
        "adjacency_matrix = lil_matrix((num_tweets, num_tweets), dtype=np.float32)\n",
        "\n",
        "for parent_id, children_ids in graph_data.items():\n",
        "    parent_idx = tweet_id_to_index.get(parent_id)\n",
        "    if parent_idx is not None:\n",
        "        for child_id in children_ids:\n",
        "            child_idx = tweet_id_to_index.get(child_id)\n",
        "            if child_idx is not None:\n",
        "                adjacency_matrix[parent_idx, child_idx] = 1\n",
        "\n",
        "\n",
        "adjacency_matrix_csr = adjacency_matrix.tocsr()\n",
        "\n",
        "\n",
        "save_npz(os.path.join(output_dir, 'adjacency_matrix16.npz'), adjacency_matrix_csr)\n",
        "\n",
        "\n",
        "adjacency_matrix_csr = load_npz(os.path.join(output_dir, 'adjacency_matrix16.npz'))\n",
        "\n",
        "\n",
        "rows, cols = adjacency_matrix_csr.nonzero()\n",
        "edge_index = np.vstack((rows, cols))\n",
        "edge_weight = adjacency_matrix_csr.data\n",
        "\n",
        "\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "\n",
        "\n",
        "embeddings_path = os.path.join(output_dir, 'combined_embeddings_matrix16.h5')\n",
        "if not os.path.exists(embeddings_path):\n",
        "    embedding_dim = 128\n",
        "    embeddings_matrix = np.random.rand(num_tweets, embedding_dim).astype(np.float32)\n",
        "    with h5py.File(embeddings_path, 'w') as hf:\n",
        "        hf.create_dataset('embeddings', data=embeddings_matrix)\n",
        "    print(f\"Embeddings matrix generated and saved to {embeddings_path}\")\n",
        "else:\n",
        "    with h5py.File(embeddings_path, 'r') as hf:\n",
        "        embeddings_matrix = hf['embeddings'][:]\n",
        "\n",
        "if embeddings_matrix.shape[0] != num_tweets:\n",
        "    raise ValueError(f\"Number of tweets in embeddings matrix ({embeddings_matrix.shape[0]}) does not match the number of tweets in adjacency matrix ({num_tweets}).\")\n",
        "\n",
        "node_features = torch.tensor(embeddings_matrix, dtype=torch.float32)\n",
        "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_features, 64)\n",
        "        self.conv2 = GCNConv(64, out_features)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_gcn = GCN(in_features=node_features.shape[1], out_features=128)\n",
        "\n",
        "\n",
        "model_gcn.eval()\n",
        "with torch.no_grad():\n",
        "    node_embeddings_gcn = model_gcn(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "print(\"Output embeddings shape for GCN:\", node_embeddings_gcn.shape)\n",
        "\n",
        "\n",
        "node_embeddings_gcn_np = node_embeddings_gcn.cpu().numpy()\n",
        "np.save(os.path.join(output_dir, 'node_embeddings_gcn16.npy'), node_embeddings_gcn_np)\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_features, 64, heads=2, concat=True)\n",
        "        self.conv2 = GATConv(64 * 2, out_features, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x, (edge_index, edge_weight) = self.conv1(x, edge_index, edge_weight, return_attention_weights=True)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return x, edge_weight\n",
        "\n",
        "model_gat = GAT(in_features=node_embeddings_gcn.shape[1], out_features=128)\n",
        "\n",
        "\n",
        "model_gat.eval()\n",
        "with torch.no_grad():\n",
        "    node_embeddings_gat, att_weights = model_gat(node_embeddings_gcn, data.edge_index, data.edge_attr)\n",
        "\n",
        "print(\"Output embeddings shape for GAT:\", node_embeddings_gat.shape)\n",
        "\n",
        "\n",
        "node_embeddings_gat_np = node_embeddings_gat.cpu().numpy()\n",
        "np.save(os.path.join(output_dir, 'node_embeddings_gat16.npy'), node_embeddings_gat_np)\n",
        "\n",
        "gat_attention_graph = np.dot(att_weights.detach().cpu().numpy(), att_weights.detach().cpu().numpy().T)\n",
        "\n",
        "print(\"\\nGAT Attention Graph:\")\n",
        "print(gat_attention_graph)\n",
        "\n",
        "\n",
        "np.save(os.path.join(output_dir, 'gat_attention_graph16.npy'), gat_attention_graph)\n",
        "\n",
        "\n",
        "def load_text_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = file.readlines()\n",
        "        print(f\"Successfully loaded text file: {file_path}\")\n",
        "        return [line.strip() for line in data]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading text file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "embeddings_gcn = verify_and_load(gcn_file_path)\n",
        "\n",
        "\n",
        "labels = load_text_file(labels_file_path)\n",
        "tweets = load_text_file(source_tweets_file)\n",
        "\n",
        "\n",
        "if embeddings_gcn is None:\n",
        "    raise RuntimeError(\"One or more embeddings could not be loaded. Check file paths and permissions.\")\n",
        "\n",
        "valid_labels = {\"false\", \"true\"}\n",
        "filtered_indices = [i for i, label in enumerate(labels) if label.split(':')[0] in valid_labels]\n",
        "\n",
        "\n",
        "embeddings_gcn = embeddings_gcn[filtered_indices]\n",
        "labels = [labels[i] for i in filtered_indices]\n",
        "tweets = [tweets[i] for i in filtered_indices]\n",
        "\n",
        "\n",
        "label_map = {\"false\": 0, \"true\": 1}\n",
        "labels = np.array([label_map[label.split(':')[0]] for label in labels])\n",
        "\n",
        "\n",
        "def load_bert_embeddings(embeddings_dir, indices):\n",
        "    embeddings = []\n",
        "    for i in indices:\n",
        "        embeddings.append(np.load(os.path.join(embeddings_dir, f'tweet_{i}_embeddings.npy')))\n",
        "    return np.array(embeddings)\n",
        "\n",
        "embeddings_bert = load_bert_embeddings(embeddings_dir, filtered_indices)\n",
        "\n",
        "\n",
        "min_samples = min(embeddings_bert.shape[0], embeddings_gcn.shape[0])\n",
        "embeddings_bert = embeddings_bert[:min_samples]\n",
        "embeddings_gcn = embeddings_gcn[:min_samples]\n",
        "labels = labels[:min_samples]\n",
        "\n",
        "\n",
        "embeddings_bert = embeddings_bert.reshape(min_samples, -1)\n",
        "embeddings_gcn = embeddings_gcn.reshape(min_samples, -1)\n",
        "\n",
        "\n",
        "def normalize(tensor):\n",
        "    return (tensor - tensor.mean()) / tensor.std()\n",
        "\n",
        "embeddings_bert = normalize(torch.tensor(embeddings_bert, dtype=torch.float32))\n",
        "embeddings_gcn = normalize(torch.tensor(embeddings_gcn, dtype=torch.float32))\n",
        "labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "def check_for_nan_inf(tensor, name):\n",
        "    if torch.isnan(tensor).any():\n",
        "        print(f\"{name} contains NaN values.\")\n",
        "    if torch.isinf(tensor).any():\n",
        "        print(f\"{name} contains Inf values.\")\n",
        "\n",
        "check_for_nan_inf(embeddings_bert, \"embeddings_bert\")\n",
        "check_for_nan_inf(embeddings_gcn, \"embeddings_gcn\")\n",
        "check_for_nan_inf(labels, \"labels\")\n",
        "\n",
        "\n",
        "class InconsistencyModule(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(InconsistencyModule, self).__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text_features):\n",
        "\n",
        "        text_self_attn_output, _ = self.self_attention(text_features, text_features, text_features)\n",
        "        text_self_attn_output = self.dropout(text_self_attn_output)\n",
        "        text_features = self.norm1(text_features + text_self_attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(text_features)\n",
        "        ffn_output = self.dropout(ffn_output)\n",
        "        output = self.norm2(text_features + ffn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GATModel, self).__init__()\n",
        "        self.conv1 = GATConv(in_features, 64, heads=2, concat=True)\n",
        "        self.conv2 = GATConv(64 * 2, out_features, heads=1, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return x\n",
        "\n",
        "class FakeNewsDetectionModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads, d_ff, dropout):\n",
        "        super(FakeNewsDetectionModel, self).__init__()\n",
        "        self.embedding_projection = nn.Linear(input_dim, d_model)\n",
        "        self.inconsistency_module = InconsistencyModule(d_model, n_heads, d_ff, dropout)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.gat_dep = GATModel(d_model, d_model)\n",
        "        self.gat_pmi = GATModel(d_model, d_model)\n",
        "        self.gat_gcn = GATModel(d_model, d_model)\n",
        "\n",
        "    def forward(self, bert_embeddings, dep_graph, pmi_graph, gcn_graph):\n",
        "\n",
        "        bert_embeddings_proj = self.embedding_projection(bert_embeddings)\n",
        "\n",
        "\n",
        "        x = self.gat_pmi(bert_embeddings_proj, pmi_graph.edge_index, pmi_graph.edge_attr)\n",
        "        v = self.gat_dep(bert_embeddings_proj, dep_graph.edge_index, dep_graph.edge_attr)\n",
        "        p = self.gat_gcn(bert_embeddings_proj, gcn_graph.edge_index, gcn_graph.edge_attr)\n",
        "\n",
        "\n",
        "        output = self.inconsistency_module(x + v + p)\n",
        "\n",
        "\n",
        "        output = self.classifier(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "model = FakeNewsDetectionModel(input_dim, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "\n",
        "\n",
        "input_dim = embeddings_bert.shape[1]\n",
        "\n",
        "d_model_values = [384, 512, 768, 1024]\n",
        "n_heads_values = [2, 4, 8, 10, 12]\n",
        "d_ff_values = [1024, 1536, 2048, 3072, 4096]\n",
        "dropout_values = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "learning_rates = [1e-5, 3e-5, 5e-5, 7e-5]\n",
        "\n",
        "def create_graph_data(matrix_path, num_nodes):\n",
        "    matrix = load_npz(matrix_path)\n",
        "    edge_index = torch.tensor(matrix.nonzero(), dtype=torch.long)\n",
        "    edge_attr = torch.tensor(matrix.data, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n",
        "    edge_index = edge_index[:, mask]\n",
        "    edge_attr = edge_attr[mask]\n",
        "\n",
        "    return Data(edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "num_nodes = embeddings_bert.shape[0]\n",
        "\n",
        "dep_graph = create_graph_data(dependency_matrix_path, num_nodes)\n",
        "pmi_graph = create_graph_data(pmi_matrix_path, num_nodes)\n",
        "\n",
        "\n",
        "gcn_edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t()\n",
        "gcn_edge_attr = torch.ones(num_nodes, dtype=torch.float32)\n",
        "gcn_graph = Data(edge_index=gcn_edge_index, edge_attr=gcn_edge_attr)\n",
        "\n",
        "\n",
        "device = torch.device('cpu')\n",
        "embeddings_bert = embeddings_bert.to(device)\n",
        "labels = labels.to(device)\n",
        "dep_graph = dep_graph.to(device)\n",
        "pmi_graph = pmi_graph.to(device)\n",
        "gcn_graph = gcn_graph.to(device)\n",
        "\n",
        "\n",
        "def train_and_evaluate(d_model, n_heads, d_ff, dropout, lr):\n",
        "\n",
        "    model = FakeNewsDetectionModel(input_dim, d_model, n_heads, d_ff, dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(embeddings_bert, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    early_stopping_patience = 5\n",
        "    best_test_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    num_epochs = 100\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(X_train, dep_graph, pmi_graph, gcn_graph)\n",
        "        train_loss = criterion(outputs, y_train)\n",
        "        train_losses.append(train_loss.item())\n",
        "\n",
        "        train_predictions = (outputs > 0.5).float()\n",
        "        train_accuracy = (train_predictions == y_train).float().mean()\n",
        "        train_accuracies.append(train_accuracy.item())\n",
        "\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_test, dep_graph, pmi_graph, gcn_graph)\n",
        "            test_loss = criterion(test_outputs, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            test_predictions = (test_outputs > 0.5).float()\n",
        "            test_accuracy = (test_predictions == y_test).float().mean()\n",
        "            test_accuracies.append(test_accuracy.item())\n",
        "\n",
        "\n",
        "        if test_loss.item() < best_test_loss:\n",
        "            best_test_loss = test_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                break\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        final_outputs = model(X_test, dep_graph, pmi_graph, gcn_graph)\n",
        "        final_predictions = (final_outputs > 0.5).float()\n",
        "        final_accuracy = (final_predictions == y_test).float().mean()\n",
        "\n",
        "    return {\n",
        "        'd_model': d_model,\n",
        "        'n_heads': n_heads,\n",
        "        'd_ff': d_ff,\n",
        "        'dropout': dropout,\n",
        "        'learning_rate': lr,\n",
        "        'final_accuracy': final_accuracy.item(),\n",
        "        'classification_report': classification_report(y_test.numpy(), final_predictions.numpy(), target_names=['False', 'True'], output_dict=True),\n",
        "        'train_losses': train_losses,\n",
        "        'test_losses': test_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'test_accuracies': test_accuracies\n",
        "    }\n",
        "\n",
        "\n",
        "parameter_combinations = list(itertools.product(d_model_values, n_heads_values, d_ff_values, dropout_values, learning_rates))\n",
        "\n",
        "\n",
        "results = []\n",
        "for params in parameter_combinations:\n",
        "    d_model, n_heads, d_ff, dropout, lr = params\n",
        "    if d_model % n_heads != 0:\n",
        "        continue\n",
        "    print(f\"Running experiment with d_model={d_model}, n_heads={n_heads}, d_ff={d_ff}, dropout={dropout}, lr={lr}\")\n",
        "    result = train_and_evaluate(d_model, n_heads, d_ff, dropout, lr)\n",
        "    results.append(result)\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel('model_experiment_results_with_epochs2.xlsx', index=False)\n",
        "print(\"Results saved to model_experiment_results_with_epochs2.xlsx\")\n",
        "\n",
        "\n",
        "def plot_results(result):\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(result['train_losses'], label='Training Loss')\n",
        "    plt.plot(result['test_losses'], label='Testing Loss')\n",
        "    plt.title('Training and Testing Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(result['train_accuracies'], label='Training Accuracy')\n",
        "    plt.plot(result['test_accuracies'], label='Testing Accuracy')\n",
        "    plt.title('Training and Testing Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "results_df = pd.read_excel('model_experiment_results_with_epochs2.xlsx')\n",
        "\n",
        "\n",
        "example_result = results_df.iloc[0]\n",
        "plot_results(example_result)\n",
        "\n"
      ]
    }
  ]
}